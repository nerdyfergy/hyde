---
layout: post
title: What is AI?: Defining terms and explaining basic concepts
---

Before we get to the question of,  "What is AI?", I feel like we should answer the big question of, "Why should we care?" I'll answer this in general terms, and then explain why attorneys and lawmakers in particular should care. 

We have all seen or read science fiction stories where an AI (usually incorporated into a robot of some kind) kills all of the humans, or we get our bodies harvested for some reason and have to fight a war with against the AI. We all leave the theater or put the book back on the shelf and go on with our lives and in the back of our mind we know that will never happen because it is just a story and that type of technology is not possible. Occasionally, we will see a news story where a scientist, usually someone famous like Stephen Hawking, will make a prediction about AI and how dangerous it is. We read it and ignore it, because in our mind we know that will never happen because that type of technology is not possible.

The back of our mind does not know what it is talking about. We have been conditioned to disbelieve AI as a work of science fiction and we might be making the biggest mistake in human history. Look at who is saying what about this subject and silence the back of your mind and just listen to these experts for a minute.
	Stephen Hawking
	Bill Gates
	Elon Musk
	Altman
	Scientist dudes
	
These people are either experts in the field of Artificial Intelligence, or they are experts in the technology industry that understand what is being worked on right now. Let all of that sink in. AI is real, it is coming, and it will have an impact, for better or for worse on all of humanity.
As law makers and policy makers, how often have you seen leaders of a new technology come out and ask to be regulated? To say that it is not something we should do, but something we must do? Most industries hate regulation, but this one welcomes it because they know the stakes and it should be taken seriously by our industry.
Usually the law is slow to react to new innovations and issues. We tend to find (latin for reactive) solutions after damage has been done, but here, waiting could be irreparable.  Now is the perfect time for the law to get involved in AI research and development. For reasons I will discuss below, we are still years away from "dangerous" AI, so there will be time for us to develop strong relationships within the industry and make a real impact. If we wait until science is on the cusp of achieving the breakthrough to the type of AI that scientists and tech leaders are afraid of now, it will be too late to have a positive impact and we might not be able to recover.
I understand how crazy this all sounds right now, but there is extremely strong evidence that those I quoted above are right. It is worth it to assess their concerns and look at how we can work together. The next section will try to explain what AI is and why many consider the threat to be real. 

It easy to start explaining what something is by explaining what the something is not. AI does not mean robot, AI is not a robot (but it can power a robot or be found in a robot), AI is not "good" or "evil", and AI is not science fiction. 
For now, lets focus on three different levels of AI, weak AI, strong AI, and Artificial Super Intelligence.
"as soon as it works, no one calls it AI anymore." - John McCarthy (McCarthy coined the term Artificial Intelligence)
Weak AI - Weak AI  falls under McCarthy's above statement. The things that work behind the scenes to identify email spam, help us to search the web, and the voice when the computer or our phones talk back to us are all Weak Ai and I bet that you have never once heard someone refer to those products and services as Artificial Intelligence. This goes to demonstrate that Weak AI is prevalent, used everyday by millions, and is largely harmless to society. 
Weak AI is built for one thing and one thing only. Whatever it was programed to do, that is all that it can do. 
Weak AI is the part of the AI industry that is booming right now and we are getting better at creating more advanced concepts. The self-driving car is one example of a Weak AI can have a recognized impact on the everyday life of the average person.  Weak Ais are used at the governmental and enterprise level as well, which most people don't ever see or think about, but the military, manufacturing, and finance industries all use sophisticated Weak AI to accomplish some specific goal or task.

As a result of its limited nature, Weak AI poses a relatively small threat to society as a whole. For example, in the 2010 Flash Crash, Weak AI was involved in the stock market losing 600 points in about 5 minutes. A Weak AI, however, does not really have the capability to "choose" to crash the stock market, but it can produce an unexpected (to us) result that may lead to limited consequences. 

Until recently, Weak AI has been limited to being developed by companies with huge budgets for research and they are able to afford expensive and extensive quality assurance checks. Additionally, these companies know that they are susceptible to huge fines or consequences if their Weak AI were to fail or deliver an unexpected result that caused damages, so they take extra precautions in their research.  
However, wealthy corporations are  not and will not remain the sole producers of Weak AI. Recently, the famous hacker 'geohot' (George Hotz) demonstrated his homemade self-driving car, which he claims that he built for $50,000 (including the cost of the car) and using his own understanding of AI programing and research. OpenAI is a non-profit artificial intelligence research company with $1 Billion in funding that will be publishing results papers, codes, and patents for anyone to use and build their incorporate into their own projects. 
Strong AI (Artificial General Intelligence) - Strong AI is where we start to get into science fiction AI. It is science fiction AI for two reasons. The first reason is that we have yet to create a Strong AI, even something like IBM's Watson is still a Weak AI. The second reason is that, when we do develop a Strong AI, it will be roughly as intelligent as the average human. This is also where definitions can get  a little murky and we can get lost trying to figure out "how do we say something is intelligent?" 
We obviously know that Weak Ais can surpass human intelligence at their specific task. In 1997, Kasparov lost to Deep Blue (unless you agree with Kasparov, but that is a different story). Weak AI have been capable of besting humans at the Weak AI's "one thing" for some time now. When we talk about as intelligent as the average human, we that everything that makes a human "intelligent"  will make the AI "intelligent" as well. Comparing Weak AI to Strong AI and questions about how we will know that we have a Strong AI are a lengthy separate topic, but just to give you an example, lets look at chess.
Deep Blue was a Weak AI that used its processing power and programming algorithms to analyze the worth of the game pieces, position of the pieces, the safety of the King, and control of the board. Each of these are evaluated and Deep Blue would try every combination possible, generates the new possibilities, then chooses the best possible move. It is just running a lot of calculations to determine the best outcome. It isn't really "thinking" about the move like a human would. A Strong AI version of Deep Blue would actually understand the consequences of each move and be able to explain those consequences to you in its own terms. 

The question about how dangerous Strong AI could be can't really be answered with any degree of accuracy. 


